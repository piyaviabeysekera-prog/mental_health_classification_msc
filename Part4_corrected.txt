Part 4 — Data Preparation (Corrected to match repository implementation)

This section documents the exact data preparation, preprocessing and validation steps used in the project. The wording below is aligned to the source code in `code/` and lists file paths and parameter values that you can cite directly in the thesis.

1. Data sources and paths
- Primary merged dataset: `data_stage/emoma_csv/merged.csv` (configured by `code/config.py` → `MERGED_CSV_PATH`).
- Enriched feature dataframe (after composite features): `data_stage/features/merged_with_composites.parquet` (generated in Phase B and used by later phases).

2. Reproducibility
- Default random seed: `RANDOM_SEED = 42` (see `code/config.py`).
- All code uses project-level seed utilities (`code/utils.py::set_global_seeds()`) before training and evaluation to enable deterministic behaviour where applicable.

3. Physiological sanity ranges (used for QC/EDA)
- The project centralises a set of physiologically plausible bounds in `code/config.py` under `PHYSIO_RANGES`:
  - `EDA_min = 0.0` (microsiemens)
  - `TEMP_min = 30.0` (°C)
  - `TEMP_max = 40.0` (°C)
  - `BVP_freq_min = 0.8` (Hz, ≈48 bpm)
  - `BVP_freq_max = 2.5` (Hz, ≈150 bpm)

4. Feature selection and enrichment
- Composite features (SRI, RS, PL etc.) are created in Phase B (`code/composites.py`) and merged with base features to produce the enriched frame `merged_with_composites.parquet` used downstream.
- Feature columns used for modelling exclude `subject` and `label`.

5. Missing-value handling (imputation)
- Imputation strategy: per-fold mean imputation using `sklearn.impute.SimpleImputer(strategy='mean')`.
- Important: imputation is fitted only on the training fold within each LOSO (or CV) fold and then applied to the test split — this prevents data leakage. The per-fold imputers are saved to `models/scalers/imputer_fold_*.pkl` (see `code/baselines.py` for the per-fold training loop).

6. Scaling
- Scaling uses `sklearn.preprocessing.StandardScaler` fitted only on the training fold and applied to test fold (per-fold scalers saved as `models/scalers/scaler_fold_*.pkl`). This also prevents leakage between training and evaluation.

7. Cross-validation & evaluation strategy
- Primary evaluation: Leave-One-Subject-Out (LOSO). LOSO fold creation and per-fold training/evaluation are implemented in `code/baselines.py` (function `_build_loso_folds()` and `_train_scalers_and_models()`).
- LOSO ensures that all windows belonging to the held-out subject are excluded from training and used only for testing in that fold.

8. Baseline models
- Baselines are trained per-fold (examples include Logistic Regression and Random Forest). Models are saved under `models/baselines/` (e.g., `logreg_fold_*.pkl`, `rf_fold_*.pkl`).
- Evaluation metrics captured per fold: `f1_macro`, `auroc_macro`, `pr_auc_macro` and are written to `reports/tables/loso_baselines.csv`.

9. Ensemble modelling, calibration & tiers
- Ensemble composition: a soft-voting ensemble of RandomForest and XGBoost using `sklearn.ensemble.VotingClassifier(voting='soft')` (see `code/ensembles.py`).
- RandomForest hyper-parameters (as used in ensemble code): `n_estimators=300`, `class_weight='balanced_subsample'`, `random_state=42`.
- Calibration: per-fold probability calibration is applied using `sklearn.calibration.CalibratedClassifierCV` with `method='isotonic'` and `cv=3` (see `_run_ensembles_and_calibration()` in `code/ensembles.py`). Calibrated models are saved per fold under `models/ensembles/`.
- Calibration evaluation: pre/post calibration Brier scores and reliability diagrams are generated (saved to `reports/figures/calibration_plots.png`) and calibration statistics are written to `reports/tables/calibration.csv`.
- Risk tiering: calibrated probabilities for the high-risk class (label `2`) are bucketed into low/medium/high tiers and per-tier statistics (sample counts, mean probability, expected cost using the code’s cost mapping) are recorded in `reports/tables/tiers_costs.csv`.

10. Explainability (SHAP) and sensitivity analysis
- Explainability uses a RandomForest explainability model (trained and saved under `models/explainability/rf_explainability_model.joblib`) and SHAP TreeExplainer to compute global feature importances (`shap.TreeExplainer` and `explainer.shap_values(...)`).
- SHAP outputs: `reports/tables/shap_top_features.csv` (ranked features by mean absolute SHAP) and a bar plot `reports/figures/shap_global.png`.
- Sensitivity analysis removes feature families and records the change in macro F1 (saved to `reports/tables/sensitivity.csv` and plotted as `reports/figures/sensitivity_spider.png`). Implementation is in `code/explainability.py`.

11. Fairness & uncertainty proxy
- The fairness analysis trains a probabilistic RandomForest and computes out-of-fold (OOF) probability estimates using `sklearn.model_selection.cross_val_predict(..., method='predict_proba')` with a 5-fold StratifiedKFold (`StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)`). The RF builder used for fairness uses `n_estimators=400` and `random_state=RANDOM_SEED` (see `code/fairness_packaging.py`).
- For each sample we store:
  - `prob_class2`: the predicted probability for the high-risk class (label `2`).
  - `pred_label`: argmax of predicted probabilities.
- Per-subject aggregation (the uncertainty proxy) computes:
  - `n_windows`, `subject_accuracy`, `mean_prob_class2`, `var_prob_class2`, `std_prob_class2`, `mean_true_label` and is written to `reports/tables/fairness_summary.csv`.

12. Reject-option analysis
- Reject bands: default bands tested are `(0.45, 0.55)`, `(0.40, 0.60)`, `(0.35, 0.65)`.
- For each band the code computes coverage and error rates both with and without rejecting ambiguous samples and writes results to `reports/tables/reject_stats.csv` (see `code/fairness_packaging.py::compute_reject_stats()`).

13. Packaging for thesis
- Final curated tables and figures are copied into:
  - `reports/tables/thesis_final/`
  - `reports/figures/thesis_final/`
- The packaging function writes `reports/tables/manifest.json` with `generated_at_utc`, `git_hash` (best-effort), and lists of included files. Implementation in `code/fairness_packaging.py::package_for_thesis()`.

14. Outputs you can cite directly (file list)
- Tables (reports/tables/):
  - `loso_baselines.csv`
  - `ensembles_per_fold.csv`
  - `calibration.csv`
  - `tiers_costs.csv`
  - `feature_family_ablation.csv`
  - `sensitivity.csv`
  - `shap_top_features.csv`
  - `fairness_summary.csv`
  - `reject_stats.csv`
  - `manifest.json` (reports/tables/manifest.json)
- Figures (reports/figures/):
  - `eda_distributions.png`
  - `corr_heatmap.png`
  - `composites_by_label.png`
  - `feature_family_bar.png`
  - `calibration_plots.png`
  - `roc_pr_curves.png` (if produced)
  - `shap_global.png`
  - `sensitivity_spider.png`

15. Notes & caveats (for thesis phrasing)
- Emphasise leakage prevention: imputation and scaling are performed per-fold (trained on the training split only) — this is critical to avoid optimistic estimates.
- Calibration details: the repository uses isotonic calibration (`CalibratedClassifierCV(method='isotonic', cv=3)`) in the ensemble pipeline; if you prefer to reference 'Platt scaling (sigmoid)' or different `cv` settings, call out that these are alternative options and not the ones used here.
- Fairness/uncertainty proxy: the metric used is the per-subject variance of the model's predicted probability for the high-risk class (a heuristic proxy for stability/uncertainty across windows). Explain that this is a practical proxy and not a formal epistemic uncertainty estimate.

16. Where to find the source code for these steps
- `code/config.py` — paths, `PHYSIO_RANGES`, `MERGED_CSV_PATH`, `RANDOM_SEED`.
- `code/baselines.py` — LOSO fold creation, per-fold imputation/scaling, baseline training & shuffle controls.
- `code/ensembles.py` — ensemble construction, per-fold calibration, tier derivation, calibration plots.
- `code/explainability.py` — SHAP computations and sensitivity experiments.
- `code/fairness_packaging.py` — cross-validated OOF probabilities, per-subject aggregation, reject-option analysis, and packaging.

17. Suggested short citation snippet for the thesis methods section
"Data were preprocessed using per-fold mean imputation (`SimpleImputer(strategy='mean')`) and standard scaling (`StandardScaler`) fitted only on training data to avoid leakage. Models were evaluated using a Leave-One-Subject-Out (LOSO) scheme. A VotingClassifier ensemble (RandomForest + XGBoost) was calibrated using isotonic regression (`CalibratedClassifierCV(method='isotonic', cv=3)`) and risk tiers derived from calibrated high-risk probabilities. Explainability was performed with SHAP TreeExplainer on a RandomForest, and fairness/uncertainty was assessed using out-of-fold probabilistic estimates aggregated per subject (variance of class-2 probability)." — see `code/` for exact implementation.

---
Generated from repository inspection on December 7, 2025.

End of Part 4 (corrected). If you want, I can also embed exact code snippets and line ranges (for copy/paste) into this file or prepare a Markdown version; tell me which format you prefer.